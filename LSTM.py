# import numpy as np
# import pandas as pd
# import tensorflow as tf
# import tensorflow.keras as keras
# import matplotlib.pyplot as plt
# import os, random, pickle
# from sklearn.preprocessing import StandardScaler, MinMaxScaler
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout, LSTM, LSTM, BatchNormalization
# from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint

# SEED = 42
# np.random.seed(SEED)
# tf.random.set_seed(SEED)
# df = pd.read_excel("/content/Data.xlsx")

# # Hyperparameters
# LR = 1e-3
# EPOCHS = 50
# BATCH_SIZE = 64
# SEQ_LEN = 60  # Same as TIME_STEPS
# UNITS = 64

# # Define a function to draw time_series plot
# def timeseries (x_axis, y_axis, x_label, y_label):
#     plt.figure(figsize = (10, 6))
#     plt.plot(x_axis, y_axis)
#     plt.xlabel(x_label, {'fontsize': 12}) 
#     plt.ylabel(y_label, {'fontsize': 12})

# timeseries(df.index, df['X1'], 'Idx','Queue Length')

# # Split train, validation and test data
# train_size = int(len(df)*0.7)
# test_val_size = int(len(df)*0.15)
# train_dataset, valid_dataset, test_dataset = df.iloc[:train_size], df.iloc[train_size:train_size+test_val_size], df.iloc[train_size+test_val_size:] 
# print('Dimension of train data: ',train_dataset.shape)
# print('Dimension of validation data: ', valid_dataset.shape)
# print('Dimension of test data: ', test_dataset.shape)

# # Split train data to X and y
# X_train = train_dataset[['sin_time', 'cos_time', 'QL', 'HOL', 'Day', 'Week']]
# y_train = train_dataset[['X1']]

# # Split valid data to X and y
# X_valid = valid_dataset[['sin_time', 'cos_time', 'QL', 'HOL', 'Day', 'Week']]
# y_valid = valid_dataset[['X1']]

# # Split test data to X and y
# X_test = test_dataset[['sin_time', 'cos_time', 'QL', 'HOL', 'Day', 'Week']]
# y_test = test_dataset[['X1']]

# # Different scaler for input and output
# scaler_x = MinMaxScaler(feature_range = (0,1))
# scaler_y = MinMaxScaler(feature_range = (0,1))

# # Fit the scaler using available training data
# input_scaler = scaler_x.fit(X_train)
# output_scaler = scaler_y.fit(y_train)

# # Apply the scaler to training data
# train_y_norm = output_scaler.transform(y_train)
# train_x_norm = input_scaler.transform(X_train)

# # Apply the scaler to valid data
# valid_y_norm = output_scaler.transform(y_valid)
# valid_x_norm = input_scaler.transform(X_valid)

# # Apply the scaler to test data
# test_y_norm = output_scaler.transform(y_test)
# test_x_norm = input_scaler.transform(X_test)

# def create_dataset (X, y, time_steps = 1):
#     Xs, ys = [], []
#     for i in range(len(X)-time_steps):
#         v = X[i:i+time_steps, :]
#         Xs.append(v)
#         ys.append(y[i+time_steps])
#     return np.array(Xs), np.array(ys)

# X_test, y_test = create_dataset(test_x_norm, test_y_norm, SEQ_LEN)
# X_valid, y_valid = create_dataset(valid_x_norm, valid_y_norm, SEQ_LEN)
# X_train, y_train = create_dataset(train_x_norm, train_y_norm, SEQ_LEN)
# print('X_train.shape: ', X_train.shape)
# print('y_train.shape: ', y_train.shape)
# print('X_valid.shape: ', X_valid.shape)
# print('y_valid.shape: ', y_valid.shape)
# print('X_test.shape: ', X_test.shape)
# print('y_test.shape: ', y_train.shape)
# model = Sequential()
# model.add(LSTM(UNITS, input_shape=(X_train.shape[1:]), return_sequences=True))
# model.add(Dropout(0.2))
# model.add(BatchNormalization())
# model.add(LSTM(UNITS))
# model.add(Dropout(0.2))
# model.add(BatchNormalization())
# model.add(Dense(1))
# model.summary()

# # Callback for early stopping
# es_callback = keras.callbacks.EarlyStopping(
#     monitor='val_loss',
#     patience=20,
#     restore_best_weights=True
# )

# opt = tf.keras.optimizers.Adam(lr=LR, decay=1e-6)

# # Compile model
# model.compile(
#     optimizer=opt,
#     loss='mae',
#     metrics=[tf.keras.metrics.MeanAbsoluteError()]
# )

# history = model.fit(
#     X_train, y_train,
#     epochs=45,
#     batch_size=BATCH_SIZE,
#     validation_data=(X_valid, y_valid),
#     shuffle=True)

# # Plot train loss and validation loss
# def plot_loss(history):
#     plt.figure(figsize = (10, 6))
#     plt.plot(history.history['loss'])
#     plt.plot(history.history['val_loss'])
#     plt.ylabel('Loss')
#     plt.xlabel('epoch')
#     plt.legend(['Train loss', 'Validation loss'], loc='upper right')

# plot_loss(history)
# y_train = scaler_y.inverse_transform(y_train)
# y_valid = scaler_y.inverse_transform(y_valid)
# y_test = scaler_y.inverse_transform(y_test)

# def prediction(model):
#     prediction = model.predict(X_test)
#     prediction = scaler_y.inverse_transform(prediction)
#     return prediction

# prediction_lstm = prediction(model)


# def plot_future(prediction, y_test):
#     plt.figure(figsize=(10, 6))
#     range_future = len(prediction)
#     plt.plot(np.arange(range_future), np.array(y_test), label='True X1')     
#     plt.plot(np.arange(range_future),np.array(prediction), label='Prediction')
#     plt.legend(loc='upper left')
#     plt.xlabel('Idx')
#     plt.ylabel('X1')

# plot_future(prediction_lstm, y_test)

# def evaluate_prediction(predictions, actual):
#     errors = predictions - actual
#     mse = np.square(errors).mean()
#     rmse = np.sqrt(mse)
#     mae = np.abs(errors).mean()
#     print('Mean Absolute Error: {:.4f}'.format(mae))
#     print('Root Mean Square Error: {:.4f}'.format(rmse))

# evaluate_prediction(prediction_lstm, y_test)


# plotting the graph
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
mae_seq = [4.4596753818498875, 5.825574205013597, 3.854307635679824, 3.177429736002192, 4.585220288045091, 4.124798007181126, 6.134493480464682, 5.826168347611286, 9.560051312609382, 1.9820842029037036, 3.749890645833552, 2.0955871470567584, 1.97934092572672, 6.589310002317784, 4.898981057977401, 2.5523440544549167, 5.869905793496606, 3.9304169654477836, 4.82284818803384, 1.8383244899993247, 4.164729144561212, 2.417617780485157, 4.688460251673935, 6.1482282023490935, 5.840089666125844, 2.936992688629352, 5.967923825374145, 5.442745296349085, 1.7345922293252503, 4.1397763979132876, 6.860755391293934, 5.088041849966768, 7.080547926806994, 1.5715397158590294, 8.331448727811154, 1.4760335730983376, 6.259184520696015, 2.447003535917573, 3.9455919591501547, 9.385187955455368, 2.247869686962252, 2.8789750095053668, 6.074761513886183, 7.246857719812329, 2.2592343230689536, 3.4251853726007138, 12.299805275304005, 2.0028034698920743, 4.898155329771838, 5.351413835582167, 4.079968879584797, 3.987519424238738, 3.885192763194909, 2.9259150576676745, 3.173760761844946, 5.675057601321517, 9.803478648870508, 2.2066661326141177, 2.085117801518663, 5.184816207428892, 4.293925259430504, 3.7357655467628623, 3.469778241802092, 5.089328370955547, 6.083677852225447, 1.5327541389189903, 2.08353428360163, 6.581870067162703, 6.492396678702152, 6.238115043947155, 8.592696713767511, 7.56419729811681, 1.767673578604713, 1.978707315077645, 3.4605245212593463, 11.218083311852974, 6.008134765115551, 4.5824949639970445, 6.5787351049292555, 5.233625475049744, 3.4388367451161717, 4.103743293139083, 1.8879668618567158, 1.5430698332672057, 7.24284269872848, 3.521163448235768, 1.816568415877966, 5.913749462733613, 7.329545180457536, 1.8581130072626844, 2.746257254982697, 2.8766629099113343, 5.1375299190194355, 2.4739508493835927, 8.813260823745258, 4.064110041350628, 8.351491500496717, 5.641700452126903, 6.249384231315899, 1.8586471732621341, 7.090460997300851, 2.1856065861994645, 4.1570868285148785, 10.882227848322865, 8.991549433832583, 7.832889190117354, 9.568073286153824, 6.431584858607335, 3.1848115239541563, 6.17912336377245, 4.755340378531505, 3.329367035878097, 7.203641181491027, 3.208594376313771, 5.167097868152849, 1.881036074949616, 4.577116682259222, 4.586963206866437, 6.522841158300751, 7.790547687202629, 2.770387681952255, 2.123990855963978, 4.348979157828242, 3.6431714468359395, 6.298234150133635, 6.58173113636864, 6.0362825015585715, 1.7570906269142728, 4.9015978227751535, 6.43811711720634, 5.512973448002283, 4.025139888632555, 2.142385753245109, 6.166068955908198, 3.8624120882362316, 2.347120931084694, 2.6039212274328287, 1.6258390833845338, 3.766851946095685, 1.6613037446922838, 1.512381599237407, 4.431475416099552, 7.760102451841461, 7.159021564462423, 2.9957127234154646, 3.720604448526957, 3.254701516189477, 5.3205516105969615, 7.230414320073212, 7.625247944657521, 7.674149332198407, 6.295458737348288, 3.8809359736769506, 1.8405333470824878, 2.3477372674138706, 5.160550473728996, 4.152859762910639, 2.2234887133020504, 7.808417303302766, 3.082206782549943, 10.489022896800204, 7.492887866647282, 3.1753316433754315, 2.4687979515085647, 3.5460788415148365, 6.179977432796647, 5.506875443177575, 3.7902813403561626, 5.780257262149439, 6.00199061796493, 7.595069425752484, 4.259682870653312, 7.027595633841126, 4.433338368415414, 3.7867384447370256, 10.594226428500576, 5.521162381627811, 2.663192940682974, 3.537277222941718, 4.1987533354601325, 5.755321007938022, 2.1827425899783996, 1.8821133486514283, 2.419537512559937, 1.6451002653845133, 4.877731484939297, 10.758854791886115, 6.917460403001377, 9.725466434086595, 6.438087863964599, 2.9424159768113483, 7.4490818830538625, 2.4986825093673564, 6.886865865667324, 4.823720952061732, 8.295906396601907, 4.140471140828597, 2.658260298395649, 5.144477243427744, 1.6969137577528364, 6.615747998761423, 3.6738046957855017, 2.547740965504107, 8.972879530666216, 5.812822837054595, 1.5328476294316267, 2.8511699519508946, 2.6365003551277653, 5.022524594288365, 3.5063062585670663, 1.751720608985267, 2.8708067676902953, 4.5242443171453885, 5.721410187906605, 5.115191417511612, 2.04899207435674, 5.438379739964188, 6.907992075000412, 2.441110733060135, 7.761753270827933, 1.7209965037032655, 5.178738995901438, 8.486585783816194, 3.8996437534477715, 6.16283653127736, 4.388173188474175, 4.764675572191939, 1.96752572246497, 6.810885171929867, 7.184361972764788, 3.386853306700221, 6.056641065428953, 5.034730843671664, 3.5507499339351734, 5.789270695290266, 4.947682710232095, 6.577444076538086, 4.331232416180468, 3.024458917174497, 8.000639079167293, 2.008681525596658, 2.2816759587893247, 1.966479307248694, 3.6549191994816552, 3.010716614287687, 6.8260108538206, 5.817260453534317, 2.083485354723198, 5.19738401198645, 4.987694249321432, 4.277296603989388, 3.659339835091842, 6.5905652345492625, 8.993995887240994, 3.221709495796943, 6.788857265947218, 3.0727632012992943, 5.156230362177234, 2.3165185450376016, 1.912837486538921, 7.744051315191526, 4.175995270675957, 5.538805230110245, 4.897913306567802, 3.275787321650612, 5.6614692358098075, 2.354234391681775, 2.111254311338345, 5.663999562064712, 6.060880018824296, 6.528989116102815, 2.0239229419952056, 3.0296987535404205, 5.799959517007386, 10.520982609024967, 4.077402055982341, 2.9614711021928035, 2.6330336155437397, 2.3921363818570502, 3.697868132418062, 5.5039507329203925, 2.0197129969108687, 3.144542719103168, 5.952375590018874, 4.277086276033499, 5.054594584807384, 5.0229829777048485, 4.356846537573737, 3.5239344276797206, 5.540375941773207, 6.637458739651807, 2.736479742862266, 4.811919291853145, 3.3861276985209656, 6.121700360909727, 1.8322660621609292, 6.179707322902482, 2.828240051476341, 5.514245285914929, 4.037242994426209, 4.242368125349828, 9.058850795664686, 5.723230882940344, 3.094469932382155, 7.416700424686555, 2.707093965574929, 3.30249105716214, 4.555916997910495, 3.366540643666357, 2.1407219751220095, 10.029081529340205, 5.612826492237128, 4.8796439244784136, 6.005655920022001, 5.768652142020395, 3.8784621789475753, 5.7250091772113, 7.077259058925961, 4.888446231913075, 2.973066557204393, 2.5082474481079595, 2.3677092513190803, 4.206768658499696, 8.764172664964567, 1.727297206830375, 7.949981821894465, 1.9967398074966345, 2.6749500039266776, 3.138210840617938, 4.6626731547387195, 5.526557133869438, 4.940722356093046, 5.7654773718, 7.325698659562553, 2.2202201711552223, 6.5593331996685285, 3.3940476281162657, 6.705285116854296, 4.359468767688283, 3.8325169153875436, 5.751512430282333, 4.592167268150679, 6.270334040153389, 7.544772727131289, 6.464781538398999, 8.772927206782436, 1.9803002039920723, 2.5151077377295605, 6.344398935037879, 5.5867699154940516, 7.793841533095528, 3.772028073614181, 6.447740543876358, 6.696219696498626, 1.6501128722108687, 6.988746176448967, 3.55572187233071, 2.3874011586760275, 6.789619376288075, 8.926018775479374, 5.942155139921091, 5.831083160773633, 5.111106776136546, 3.3821924141223767, 6.4937901036405945, 2.923342729962445, 6.005789507135142, 6.388259426761141, 5.3799704545126685, 6.913349769855368, 6.740765495117966, 3.777169699620719, 5.107231848667504, 9.922246159679025, 12.333354013061523, 5.418610723529088, 8.010145856122293, 1.5830089024135046, 1.8587619375914384, 2.4023380770440395, 2.2430747350056968, 2.386206124176075, 1.5547519440674025, 6.031545847542002, 6.205199248321615, 4.1455721137205845, 1.7277424316158945, 6.187276343515333, 5.332961135641201, 7.083644707739191, 4.976468263213167, 7.277663833352938, 3.710497113010558, 5.850187846740822, 2.9684874490431286, 8.881849633366178, 3.9794050671896732, 10.97024495357891, 1.817862050808644, 1.8822221557408163, 4.221563710135807, 4.801663052047235, 6.379299441165096, 4.752963091294452, 1.6872266233459976, 2.353733917526691, 6.188231597217916, 1.6859240160257267, 4.253467405706594, 3.1714327096278647, 2.3209247144803826, 2.7906257305325304, 3.05894586150682, 11.13121345299247, 4.907524160953617, 5.315494457570388, 2.502580985085782, 3.0034158924160357, 2.154408849279642, 7.233389431822534, 5.557239516424585, 7.611429055442574, 5.92557290834681, 5.132229302126703, 7.206117670838262, 4.915760003706861, 3.8004088059850227, 4.2523498955657075, 4.6180432011464525, 6.149819493496384, 4.728780729690872, 3.11224765604291, 2.2671001980831975, 3.3087864915416807, 1.9993793520153078, 7.917941448692587, 3.6545979223681613, 1.9824909277818352, 4.638511537072586, 2.2472582830740593, 8.15400179569159, 2.3741189144864157, 4.221067645208393, 2.639934002289363, 7.596827028258046, 1.8873024542621981, 4.401845189892853, 2.0224837743870574, 6.4203975867451994, 1.9934691161003666, 5.776979474639449, 6.078885129965162, 4.316646554845929, 6.760500533537734, 4.999636286410229, 7.8195202699963, 1.936419803590401, 4.585338177354521, 5.961802597693972, 4.554109573923598, 4.770832799283183, 10.671212017501123, 3.054008749950065, 3.4504933871447374, 5.036849910319731, 3.4893485968872, 5.0880911543410186, 1.6196246810605923, 3.853504324496393, 8.344670895086905, 5.69085293586808, 2.0386921186520897, 2.056819257457885, 5.255005073319892, 3.4041759149352115, 3.131365508713982, 6.521824079955287, 3.244029961066592, 5.751715375766399, 3.2526403885345916, 1.8221103980178512, 3.240516339921578, 7.399676037237111, 2.9382317271949154, 4.840379854415478, 4.5493639838096955, 12.575933666679802, 3.9900240744873563, 9.591625413390892, 4.432607753371033, 4.726834738319847, 3.5439350400154073, 3.6908283053264888, 7.7562282366510855, 10.485068824604745, 4.571053999009048, 3.9543529506989743, 3.281652477491553, 5.182260260672824, 2.2419576275165265]
itr = []
for i in range (1,1000, 2):
  itr.append(i)
# plt.plot(itr, mae_seq)
# plt.xlabel('MAE')
# plt.ylabel('Sequence Length')
# plt.title('Time Sequence Length vs MAE')
# plt.grid()
# plt.show()
print(min(mae_seq))

# #descriptive statistics
# df = pd.DataFrame(mae_seq)
# print(df.describe().transpose())
# sns.histplot(df, kde = True, binwidth=0.5, legend=False, element='bars', stat='frequency')
# plt.xlabel("Mean Absolute Error")
# plt.grid()
# plt.show()